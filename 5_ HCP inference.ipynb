{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oZyvFNIKDKWc"},"outputs":[],"source":["root = '' # change as needed\n","data_root = root + 'data/'\n","save_path = root + 'results/inference/'\n","model_path = root + 'model' # path to the model reported in the paper (downloaded from Zenodo), you can change it to your own model\n","sc_file = data_root + 'SC_dbs80HARDIFULL.mat'\n","dbs_path = root + 'ds80_labels.csv'\n","yeo_path = root + 'dbs802Yeo7.csv'\n","!mkdir -p {save_path}"]},{"cell_type":"markdown","metadata":{"id":"9ofc61s4dBJJ"},"source":["# 1) Read"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kibLWigFRt0F"},"outputs":[],"source":["import mat73\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JwkYIeFzyQM7"},"outputs":[],"source":["def get_ts_hcp_task(data_root):\n","    '''\n","    Reads the HCP task dataset.\n","\n","    Args:\n","        data_root (str): Folder containing the data files (.mat).\n","\n","    Returns:\n","        time_series (dict): Dictionary where the keys are the tasks and the values are the arrays.\n","    '''\n","    time_series = {}\n","    time_series['memory'] = mat73.loadmat(data_root+'hcp1003_WM_LR_dbs80.mat')\n","    time_series['gambling'] = mat73.loadmat(data_root+'hcp1003_GAMBLING_LR_dbs80.mat')\n","    time_series['motor'] = mat73.loadmat(data_root+'hcp1003_MOTOR_LR_dbs80.mat')\n","    time_series['language'] = mat73.loadmat(data_root+'hcp1003_LANGUAGE_LR_dbs80.mat')\n","    time_series['social'] = mat73.loadmat(data_root+'hcp1003_SOCIAL_LR_dbs80.mat')\n","    time_series['relational'] = mat73.loadmat(data_root+'hcp1003_RELATIONAL_LR_dbs80.mat')\n","    time_series['emotion'] = mat73.loadmat(data_root+'hcp1003_EMOTION_LR_dbs80.mat')\n","    time_series['rest'] = mat73.loadmat(data_root+'hcp1003_REST1_LR_dbs80.mat')\n","    return time_series\n","\n","# Read time series\n","data_ts = get_ts_hcp_task(data_root)\n","# Get only useful data\n","for k in data_ts.keys():\n","  data_ts[k] = [i['dbs80ts'] for i in data_ts[k]['subject'] if type(i) is dict]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mjRLSaD3yQM-"},"outputs":[],"source":["# Create time series dataframe\n","rows = []\n","for cohort, values in data_ts.items():\n","    for id, value in enumerate(values):\n","        rows.append({'cohort': cohort.capitalize(), 'bold': value, 'subject_id': id})\n","del data_ts\n","time_series = pd.DataFrame(rows)\n","time_series"]},{"cell_type":"markdown","metadata":{"id":"Nz9QdSfZwBly"},"source":["# 2) Signal filtering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYt-6Qm3yQNC"},"outputs":[],"source":["from scipy.signal import butter, detrend, filtfilt\n","\n","def demean(x,dim=0):\n","    dims = x.size\n","    return x - np.tile(np.mean(x,dim), dims)  # repmat(np.mean(x,dim),dimrep)\n","\n","def BandPassFilter(boldSignal, f_low, f_high, TR, k, removeStrongArtefacts=True):\n","    # Convenience method to apply a filter (always the same one) to all areas in a BOLD signal. For a single,\n","    # isolated area evaluation, better use the method below.\n","    (N, Tmax) = boldSignal.shape\n","    fnq = 1./(2.*TR)              # Nyquist frequency\n","    Wn = [f_low/fnq, f_high/fnq]                                   # butterworth bandpass non-dimensional frequency\n","    bfilt, afilt = butter(k,Wn, btype='band', analog=False)   # construct the filter\n","    # bfilt = bfilt_afilt[0]; afilt = bfilt_afilt[1]  # numba doesn't like unpacking...\n","    signal_filt = np.zeros(boldSignal.shape)\n","    for seed in range(N):\n","        if not np.isnan(boldSignal[seed, :]).any():  # No problems, go ahead!!!\n","            ts = demean(detrend(boldSignal[seed, :]))  # Probably, we do not need to demean here, detrend already does the job...\n","\n","            if removeStrongArtefacts:\n","                ts[ts>3.*np.std(ts)] = 3.*np.std(ts)    # Remove strong artefacts\n","                ts[ts<-3.*np.std(ts)] = -3.*np.std(ts)  # Remove strong artefacts\n","\n","            signal_filt[seed,:] = filtfilt(bfilt, afilt, ts, padlen=3*(max(len(bfilt),len(afilt))-1))  # Band pass filter. padlen modified to get the same result as in Matlab\n","        else:  # We've found problems, mark this region as \"problematic\", to say the least...\n","            print(f'############ Warning!!! BandPassFilter: NAN found at region {seed} ############')\n","            signal_filt[seed,0] = np.nan\n","    return signal_filt\n","\n","def AmplitudeFilter(time_series):\n","    return time_series/np.abs(time_series).max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4SgAnXzwRPq"},"outputs":[],"source":["# Filters parameters\n","TR = 2.0\n","k = 2                                # 2nd order butterworth filter\n","f_low = 0.008                        # lowpass frequency of filter\n","f_high = 0.08                        # highpass\n","\n","# Apply filters\n","time_series.loc[:,'bold'] = time_series['bold'].apply(BandPassFilter, args=(f_low, f_high, TR, k))\n","time_series.loc[:,'bold'] = time_series['bold'].apply(AmplitudeFilter)"]},{"cell_type":"markdown","metadata":{"id":"tZVvRKHvx9Cy"},"source":["# 3) Windowing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KqlCgPLyQND"},"outputs":[],"source":["t_use = 50\n","\n","def create_windows(ts):\n","    N, t = ts.shape\n","    n_windows = t//t_use\n","    windows = ts[:, -n_windows*t_use:].reshape((n_windows, N, t_use))\n","    return windows\n","\n","# Extract windows and create a new dataframe\n","all_windows = []\n","for idx, row in time_series.iterrows():\n","    windows = create_windows(row['bold'])\n","    for i, window in enumerate(windows):\n","        all_windows.append({'cohort': row['cohort'], 'subject_id': row['subject_id'], 'window_id': i, 'window': window})\n","    all_windows.append({'cohort': row['cohort'], 'subject_id': row['subject_id'], 'window_id': 'mean', 'window': windows.mean(axis=0)})\n","del time_series\n","\n","# Create a new dataframe from the list of windows\n","windows = pd.DataFrame(all_windows)\n","del all_windows\n","windows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URUIA-huyQND"},"outputs":[],"source":["def get_filename(row):\n","    if row['window_id'] == 'mean':\n","        return f\"{row['cohort']}_{row['subject_id']}\"\n","    else:\n","        return f\"{row['cohort']}_{row['subject_id']}_{row['window_id']}\"\n","\n","# Add filenames to the df\n","windows['filename'] = windows.apply(get_filename, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"H61rPd9qyQNE"},"source":["# 4) Time-series-to-image conversion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4XlWpI8yQNE"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","\n","ratio = 77\n","def plot_ts(ts, path):\n","    N, t = ts.shape\n","    plt.figure(figsize=(t/ratio, N/ratio))\n","    plt.imshow(ts, aspect='auto', cmap='viridis', vmin=-1, vmax=1)\n","    plt.axis('off')\n","    plt.savefig(path, bbox_inches='tight', pad_inches=0)\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vNvWlAptyQNE"},"outputs":[],"source":["import tqdm, os\n","\n","if (not os.path.exists(save_path+'img/')):\n","\tos.mkdir(save_path+'img/')\n","\n","# Create and save images\n","for _, row in tqdm.tqdm(windows.iterrows(), total=len(windows)):\n","    plot_ts(row['window'], f\"{save_path}/img/{row['filename']}.png\")"]},{"cell_type":"markdown","metadata":{"id":"0yGmMtWW2aSy"},"source":["# 5) Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2KO7EBlryQNF"},"outputs":[],"source":["from fastai.vision.all import *\n","\n","a_min, a_max = -1, 1\n","\n","# Metric used\n","def rmse_a(inp, targ):\n","  return rmse(inp, targ)*100/(a_max-a_min)"]},{"cell_type":"code","source":["# Get inputs for the model\n","image_list = save_path + '/img/' + windows['filename'].values + '.png'\n","# Create dummy dataloader\n","dls = ImageDataLoaders.from_path_func('', [0], lambda x: '0', bs=16, item_tfms=Resize((80, 50), method='squish'))\n","# Load model\n","learn = vision_learner(dls, 'convnext_tiny_in22k', n_out=80, y_range=(-1,1), loss_func=MSELossFlat).to_fp16()\n","learn.load(model_path, device='cuda')\n","# Predict\n","test_dl = learn.dls.test_dl(image_list, device='cuda')\n","preds, _ = learn.get_preds(dl=test_dl)"],"metadata":{"id":"DNjAyN_p66bI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3Sq_DOsyQNF"},"outputs":[],"source":["# Add predictions to the dataframe\n","windows['pred'] = [p for p in preds.numpy()]"]},{"cell_type":"markdown","metadata":{"id":"Cey3UEyqyQNF"},"source":["# 6) Yeo networks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zaHVqG80yQNF"},"outputs":[],"source":["# Read names of the 80 ROIs\n","dbs_names = pd.read_csv(dbs_path, sep=';').loc[:,'Rois'].values\n","# Read asociated regions\n","yeo_ids = pd.read_csv(yeo_path, header=None).iloc[0].values\n","yeo_names = np.array([None, 'Visual', 'Somatomotor', 'Dorsal attention', 'Ventral attention', 'Limbic', 'Frontoparietal', 'Default'])\n","# Filter non cortical regions\n","dbs_names_cortex = dbs_names[np.r_[0:31, 49:80]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwC19mBSyQNF"},"outputs":[],"source":["from collections import defaultdict\n","\n","tasks_act = defaultdict(list)\n","for c in np.unique(windows.cohort.values):\n","    # Filter windows by task\n","    selection = np.stack(windows[windows['cohort'] == c][windows['window_id'] == 'mean'].pred.values)\n","    # Get the mean value for each ROI in a particular task\n","    activations = selection.mean(axis=0)\n","    # Filter non cortical regions\n","    activations_cortex = activations[np.r_[0:31, 49:80]]\n","    tasks_act[c].append(activations_cortex)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0TqtMPlgyQNF"},"outputs":[],"source":["comp = pd.DataFrame(columns=tasks_act.keys())\n","comp = comp.drop(columns=['Rest'])\n","\n","for c in tasks_act.keys():\n","\tif c != 'Rest':\n","\t\tnet_act = defaultdict(list)\n","\t\tactivations_cortex = tasks_act[c][0]-tasks_act['Rest'][0]\n","\t\tfor net,act in zip(yeo_names[yeo_ids], activations_cortex):\n","\t\t\tif act>-20:\n","\t\t\t\tnet_act[net].append(act)\n","\t\tfor net in net_act.keys():\n","\t\t\tcomp.loc[net, c] = [np.mean(net_act[net]), np.max(net_act[net])]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEFMPfx1yQNF"},"outputs":[],"source":["import seaborn as sns\n","\n","data = {'x': [], 'y': [], 'network': []}\n","\n","# Iterate over the df to extract y values and column/row labels\n","for col in comp.columns:\n","    for index, values in comp[col].items():\n","        data['x'].append(col)\n","        data['y'].append(values[0])\n","        data['network'].append(index)\n","\n","# Convert data into a new df\n","df_plot = pd.DataFrame(data)\n","\n","# Create scatter plot\n","plt.figure(figsize=(10, 6))\n","sns.stripplot(x='x', y='y', hue='network', data=df_plot, palette='deep', jitter=False, size=9, edgecolor='black', linewidth=0.5, alpha=0.7)\n","\n","# Plot formatting\n","plt.xlabel('Tasks', fontsize=14)\n","plt.ylabel(r'Difference of mean bifurcation parameters', fontsize=14)\n","plt.xticks(fontsize=10)\n","plt.yticks(fontsize=10)\n","plt.grid(True, which='major', linestyle='--', alpha=0.6)\n","plt.legend(title='Networks', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, title_fontsize=14)\n","plt.tight_layout()\n","plt.savefig(save_path+'networks.png', dpi=800)"]},{"cell_type":"markdown","metadata":{"id":"oo8WerHrhzhE"},"source":["# 7) Task separation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4ZXewYNzlY3"},"outputs":[],"source":["import seaborn as sns\n","from itertools import combinations\n","from statannotations.Annotator import Annotator\n","\n","def pad_dicts(tests):\n","    \"\"\"Ensure all dictionaries have the same length by padding with NaNs.\"\"\"\n","    max_len = max(len(lst) for lst in tests.values())\n","    return {key: np.pad(lst, (0, max_len - len(lst)), 'constant', constant_values=np.nan)\n","            for key, lst in tests.items()}\n","\n","def plot_comparison_across_labels(tests, method=\"BH\", title='', x_label='', y_label='', fig_size=(12, 8), save_path='./'):\n","\n","\t# Prepare data\n","\ttests = pad_dicts(tests)\n","\tuse_labels = list(tests.keys())\n","\tpairs = list(combinations(use_labels, 2))\n","\tdf = pd.DataFrame(tests, columns=use_labels)\n","\n","\t# Set up figure and axes\n","\tfig, (ax_table, ax_violin) = plt.subplots(2, 1, figsize=fig_size, gridspec_kw={'height_ratios': [1.2, 3]})\n","\tsns.set_context(\"talk\")\n","\n","\t# Perform statistical comparisons and annotate\n","\tannotator = Annotator(ax=ax_violin, data=df, pairs=pairs, order=use_labels)\n","\tannotator.configure(test='Mann-Whitney', text_format='star', verbose=True)\n","\tannotator.configure(comparisons_correction=method, correction_format=\"replace\")\n","\tresults = annotator.apply_test()\n","\n","\t# Prints\n","\tannotator.print_pvalue_legend()\n","\tfor a in results.annotations:\n","\t\ta.print_labels_and_content()\n","\n","\t# Create violin plot\n","\tsns.violinplot(data=df, order=use_labels, ax=ax_violin, palette='deep', linewidth=1.2)\n","\tax_violin.set_xlabel(x_label, fontsize=14)\n","\tax_violin.set_ylabel(y_label, fontsize=14)\n","\tax_violin.set_xticklabels(ax_violin.get_xticklabels(), fontsize=12)\n","\n","\t# Grid and despine for cleaner look\n","\tsns.despine()\n","\tax_violin.grid(True, which='major', linestyle='--', alpha=0.6)\n","\n","\t# Create a significance stars table\n","\tstars_matrix = pd.DataFrame(\"\", index=df.columns[1:], columns=df.columns[:-1])\n","\tfor result in results.annotations:\n","\t\tc1, c2 = [str(struct[\"label\"]) for struct in result.structs]\n","\t\tstars_matrix.loc[c2, c1] = result.text  # Insert stars\n","\n","\t# Plot the table above the violin plot\n","\tbbox_v = ax_violin.get_position()\n","\tnew_left = (bbox_v.x0 + bbox_v.x1) / 2 - 0.7 / 2\n","\ttable = pd.plotting.table(ax_table, stars_matrix, loc='center', cellLoc='center', fontsize=15, bbox=[new_left,0,0.7,1])\n","\tax_table.axis('off')\n","\n","\t# Title\n","\tfig.suptitle(title, fontsize=18, y=0.93)\n","\n","\t# Save plot with high resolution\n","\tplt.tight_layout()\n","\tplt.savefig(save_path, dpi=800)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8ugXXkGEN-g"},"outputs":[],"source":["# Create and plot violin plot\n","result = windows[windows['window_id'] == 'mean'].groupby('cohort')['pred'].apply(lambda x: np.mean(np.vstack(x), axis=1).tolist()).to_dict()\n","plot_comparison_across_labels(result, x_label='Cohorts', y_label=r'Mean bifurcation parameters $a$', title='', save_path=save_path+'tasks.png')"]},{"cell_type":"markdown","metadata":{"id":"69iMEOvayQNH"},"source":["# 8) FC and FCD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IR-SstjsyQNH"},"outputs":[],"source":["import scipy.io\n","\n","# Read omega vector and SC\n","W = np.load(root+'results/G/w.npy')\n","SC = scipy.io.loadmat(sc_file)['SC_dbs80FULL']\n","SC = 0.2 * SC/SC.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hGf5ZKcyQNH"},"outputs":[],"source":["# Set parameters in the df\n","windows['SC'] = windows['window'].apply(lambda x: SC)\n","windows['G'] = 2.3\n","windows['W'] = [i for i in np.repeat(W[None,:], len(windows), axis=0)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2b83Gy2yQNH"},"outputs":[],"source":["from numba import jit\n","\n","@jit(nopython=True)\n","def ode_hopf(t, vars, a=-0.02, w=1, G=0, C=0):\n","    '''\n","    Defines the ordinary differential equations of the Hopf model.\n","\n","    Args:\n","        t (int): Dummy parameter.\n","        vars (np.array): x and y variables of the model.\n","        a (np.array): Amplitude parameters of each node.\n","        w (np.array): Frequency parameters of each node.\n","        G (float): Coumpling factor constant.\n","        C (np.array): Matrix of structural connectivity.\n","\n","    Returns:\n","        dvars (np.array): Derivates of the x and y variables of the model.\n","    '''\n","    n = len(a)\n","    x = vars[:n].flatten()\n","    y = vars[n:].flatten()\n","\n","    x_term = np.dot(C,x) - C.sum(axis=1) * x\n","    y_term = np.dot(C,y) - C.sum(axis=1) * y\n","\n","    dxdt = a*x - w*y - x*(x**2 + y**2) + G*x_term\n","    dydt = a*y + w*x - y*(x**2 + y**2) + G*y_term\n","\n","    dvars = np.concatenate((dxdt, dydt), axis=0)\n","    return dvars\n","\n","def initialize_hopf(n_samples, nodes, seed, a_range, w_range, g, SC):\n","    '''\n","    Initializes Hopf parameters by:\n","        * Stacking SC and G.\n","        * Generating random values for a and w using a fixed seed.\n","    '''\n","    np.random.seed(seed=seed)\n","    A = np.random.uniform(a_range[0], a_range[1], size=(n_samples, nodes))\n","    W = np.random.uniform(w_range[0], w_range[1], size=(n_samples, nodes))\n","    G = np.repeat(g, n_samples)\n","    C = np.tile(SC[None,:], (n_samples,1,1))\n","    return A, W, G, C\n","\n","@jit(nopython=True)\n","def numba_noise(size):\n","    '''\n","    The parameter 'size' in np.random.normal() is not supported by numba, this function fixes that.\n","    '''\n","    noise = np.empty(size,dtype=np.float64)\n","    for i in range(size):\n","        noise[i] = np.random.normal()\n","    return noise\n","\n","@jit(nopython=True)\n","def integrate_hopf_euler_maruyama(A, W, C, G, TR, t_use, t_max, init_min=-1, init_max=1, dt=0.5, sigma=0.01):\n","\t'''\n","\tIntegrates the Hopf model using the Euler-Maruyama method for each provided subject.\n","\n","\tArgs:\n","\t\tA, W, C, G: Hopf parameters for each subject.\n","\t\tTR (float): Repetition time of the dataset.\n","\t\tt_use (int): Number of timesteps to return.\n","\t\tt_max (float): Last timestep.\n","\t\tinit_min (float): Minimum value for variable initialization.\n","\t\tinit_max (float): Maximum value for variable initialization.\n","\t\tdt (float): Distance between timesteps.\n","\t\tsigma (float): Controls the amount of noise.\n","\n","\tReturns:\n","\t\tx_solution (np.array): Resulting time series for each subject.\n","\t'''\n","\tn_samples, nodes = A.shape\n","\n","\t# Sample time\n","\tt = np.arange(0.0, t_max*TR, dt)\n","\n","\t# Initialize array to store the results\n","\tx_solution = np.empty((n_samples, nodes, len(t)))\n","\n","\tfor n in range(n_samples):\n","\t\t# Initial conditions for x and y for each node\n","\t\tx0 = np.random.uniform(init_min, init_max, size=nodes)\n","\t\ty0 = np.random.uniform(init_min, init_max, size=nodes)\n","\t\tvars = np.concatenate((x0, y0), axis=0)\n","\n","\t\ta = A[n]\n","\t\tw = W[n]\n","\t\tc = C[n]\n","\t\tg = G[n]\n","\n","\t\t# Euler-Maruyama integration\n","\t\tfor i in range(1, len(t)):\n","\t\t\t# Time\n","\t\t\tt_span = (t[i - 1], t[i])\n","\t\t\t# Derivates\n","\t\t\td_vars = ode_hopf(t_span, vars, a, w, g, c)\n","\n","\t\t\t# Euler-Maruyama integration\n","\t\t\tvars += d_vars*dt + np.sqrt(dt)*sigma*numba_noise(size=2*nodes)\n","\n","\t\t\t# Clamping\n","\t\t\tvars[vars > init_max] = init_max\n","\t\t\tvars[vars < init_min] = init_min\n","\n","\t\t\t# Only save values for x\n","\t\t\tx_solution[n, :, i] = vars[:nodes]\n","\n","\t\tif np.isnan(x_solution[n]).any():\n","\t\t\tprint(f'NaN found! n={n}')\n","\t\t\traise\n","\n","\t\tif (n+1) % 100 == 0:\n","\t\t\tprint(f'Generated {n+1}/{n_samples} samples')\n","\n","\t# Delete initial unwanted timesteps\n","\tx_solution = x_solution[:,:,-int((t_use*TR/dt)):]\n","\t# Fix sampling rate\n","\tx_solution = x_solution[:,:,::int(TR/dt)]\n","\n","\treturn x_solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdfSyb5fyQNI"},"outputs":[],"source":["# Integrate\n","X = integrate_hopf_euler_maruyama(np.stack(windows['pred'].values), np.stack(windows['W'].values), np.stack(windows['SC'].values), np.stack(windows['G'].values), TR, t_use, t_use+100)\n","windows['X'] = [x for x in X]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Huowvf-yQNI"},"outputs":[],"source":["# Drop unnecessary columns\n","windows.drop(columns=['SC', 'G', 'W'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KJ6rMNpyQNI"},"outputs":[],"source":["from scipy import signal\n","from numba import jit\n","\n","def demean(x,dim=0):\n","    dims = x.size\n","    return x - np.mtlib.tile(np.mean(x,dim), dims)\n","\n","@jit(nopython=True)\n","def adif(a, b):\n","    if np.abs(a - b) > np.pi:\n","        c = 2 * np.pi - np.abs(a - b)\n","    else:\n","        c = np.abs(a - b)\n","    return c\n","\n","@jit(nopython=True)\n","def numba_PIM(phases, N, Tmax, dFC, PhIntMatr, discardOffset=10):\n","  T = np.arange(discardOffset, Tmax - discardOffset + 1)\n","  for t in T:\n","    for i in range(N):\n","      for j in range(i+1):\n","        dFC[i, j] = np.cos(adif(phases[i, t - 1], phases[j, t - 1]))\n","        dFC[j, i] = dFC[i, j]\n","    PhIntMatr[t - discardOffset] = dFC\n","  return PhIntMatr\n","\n","def PhaseInteractionMatrix(ts, discardOffset=10):  # Compute the Phase-Interaction Matrix of an input BOLD signal\n","    if not np.isnan(ts).any():  # No problems, go ahead!!!\n","        (N, Tmax) = ts.shape\n","        npattmax = Tmax - (2 * discardOffset - 1)  # calculates the size of phfcd matrix\n","        # Data structures we are going to need...\n","        phases = np.empty((N, Tmax))\n","        dFC = np.empty((N, N))\n","        PhIntMatr = np.empty((npattmax, N, N))\n","\n","        for n in range(N):\n","            Xanalytic = signal.hilbert(demean(ts[n, :]))\n","            phases[n, :] = np.angle(Xanalytic)\n","\n","        PhIntMatr = numba_PIM(phases, N, Tmax, dFC, PhIntMatr)\n","\n","    else:\n","        print('############ Warning!!! PhaseInteractionMatrix.from_fMRI: NAN found ############')\n","        PhIntMatr = np.array([np.nan])\n","    # ======== sometimes we need to plot the matrix. To simplify the code, we save it here if needed...\n","    # if saveMatrix:\n","    #     import scipy.io as sio\n","    #     sio.savemat(save_file + '.mat', {name: PhIntMatr})\n","    return PhIntMatr\n","\n","def tril_indices_column(N, k=0):\n","    row_i, col_i = np.nonzero(\n","        np.tril(np.ones(N), k=k).T)  # Matlab works in column-major order, while Numpy works in row-major.\n","    Isubdiag = (col_i,\n","                row_i)  # Thus, I have to do this little trick: Transpose, generate the indices, and then \"transpose\" again...\n","    return Isubdiag\n","\n","@jit(nopython=True)\n","def numba_phFCD(phIntMatr_upTri, size_kk3):\n","    npattmax = phIntMatr_upTri.shape[0]\n","    phfcd = np.zeros((size_kk3))\n","    kk3 = 0\n","\n","    for t in range(npattmax - 2):\n","        p1_sum = np.sum(phIntMatr_upTri[t:t + 3, :], axis=0)\n","        p1_norm = np.linalg.norm(p1_sum)\n","        for t2 in range(t + 1, npattmax - 2):\n","            p2_sum = np.sum(phIntMatr_upTri[t2:t2 + 3, :], axis=0)\n","            p2_norm = np.linalg.norm(p2_sum)\n","\n","            dot_product = np.dot(p1_sum, p2_sum)\n","            phfcd[kk3] = dot_product / (p1_norm * p2_norm)\n","            kk3 += 1\n","    return phfcd\n","\n","def phFCD(ts, discardOffset=10):  # Compute the FCD of an input BOLD signal\n","    phIntMatr = PhaseInteractionMatrix(ts)  # Compute the Phase-Interaction Matrix\n","    if not np.isnan(phIntMatr).any():  # No problems, go ahead!!!\n","        (N, Tmax) = ts.shape\n","        npattmax = Tmax - (2 * discardOffset - 1)  # calculates the size of phfcd vector\n","        size_kk3 = int((npattmax - 3) * (npattmax - 2) / 2)  # The int() is not needed because N*(N-1) is always even, but \"it will produce an error in the future\"...\n","        Isubdiag = tril_indices_column(N, k=-1)  # Indices of triangular lower part of matrix\n","        phIntMatr_upTri = np.zeros((npattmax, int(N * (N - 1) / 2)))  # The int() is not needed, but... (see above)\n","        for t in range(npattmax):\n","            phIntMatr_upTri[t,:] = phIntMatr[t][Isubdiag]\n","        phfcd = numba_phFCD(phIntMatr_upTri, size_kk3,)\n","\n","    else:\n","        print('############ Warning!!! phFCD.from_fMRI: NAN found ############')\n","        phfcd = np.array([np.nan])\n","    # if saveMatrix:\n","    #     buildMatrixToSave(phfcd, npattmax - 2)\n","    return phfcd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5nPxA9ZPyQNJ"},"outputs":[],"source":["from scipy import stats\n","\n","def get_correlation(a, b):\n","    return np.corrcoef(a[np.triu_indices(a.shape[-1])], b[np.triu_indices(a.shape[-1])])[0,1]\n","\n","def get_ks_distance(a, b):\n","    d, pvalue = stats.ks_2samp(a.flatten(), b.flatten())\n","    return d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTrp5_SeyQNJ"},"outputs":[],"source":["def apply_corr(x):\n","    fc_emp = np.corrcoef(x['window'])\n","    fc_emp[np.isnan(fc_emp)] = np.nanmean(fc_emp)\n","\n","    fc_sim = np.corrcoef(x['X'])\n","    fc_sim[np.isnan(fc_sim)] = np.nanmean(fc_sim)\n","    return np.abs(get_correlation(fc_emp, fc_sim))\n","\n","# Get measures\n","windows['FC_correlation'] = windows.apply(apply_corr, axis=1)\n","windows['phFCD_distance'] = windows.apply(lambda x: get_ks_distance(phFCD(x['window']), phFCD(x['X'])), axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKMngrB_yQNJ"},"outputs":[],"source":["# Create and plot violin plot for FC\n","FC = windows[windows['window_id'] == 'mean'].groupby('cohort')['FC_correlation'].apply(list).to_dict()\n","plot_comparison_across_labels(FC, x_label='Cohorts', y_label='FC correlation', title='', save_path=save_path+'FC.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywQB9QcNyQNJ"},"outputs":[],"source":["# Explore FC values\n","pd.DataFrame([(np.mean(FC[k]), np.std(FC[k]), np.min(FC[k]), np.max(FC[k])) for k in FC.keys()], columns=['mean', 'std', 'min', 'max'], index=FC.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCNx1CB4yQNJ"},"outputs":[],"source":["# Create and plot violin plot for FCD\n","phFCD = windows[windows['window_id'] == 'mean'].groupby('cohort')['phFCD_distance'].apply(list).to_dict()\n","plot_comparison_across_labels(phFCD, x_label='Cohorts', y_label='KS distance FCD', title='', save_path=save_path+'phFCD.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E0kFNQtsyQNQ"},"outputs":[],"source":["# Explore FCD values\n","pd.DataFrame([(np.mean(phFCD[k]), np.std(phFCD[k]), np.min(phFCD[k]), np.max(phFCD[k])) for k in phFCD.keys()], columns=['mean', 'std', 'min', 'max'], index=phFCD.keys())"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}